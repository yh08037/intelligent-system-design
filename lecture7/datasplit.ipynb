{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Lecture: Data Splitting\n",
    "이번 강의에서는 기계학습에서 주어진 데이터로부터 실험 설계를 하는 방법에 대한 내용을 소개합니다. \n",
    "이 방법들을 사용하면 우리가 설계한 신경회로망이 학습에 사용되지 않은 미관측 데이터에 대한 성능을 비교적 정확히 예측할 수 있게 해 주며, 이에 따라 실제 환경에서의 성능을 예측할 수 있게 해 줍니다. \n",
    "또한 이 방법들은 신경회로망 뿐만이 아니라 다른 기계학습 방법에도 별다른 수정없이 적용가능합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Copyrights\n",
    "\n",
    "> ELEC801 Pattern Recognition, Fall 2020, Gil-Jin Jang\n",
    "\n",
    "> Lecture 4 Overfitting Avoidance\n",
    "\n",
    "---\n",
    "\n",
    "### Customized by Gil-Jin Jang, April 14, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "## 파일 설명\n",
    "| 파일명 | 파일 용도 | 관련 절 | 페이지 |\n",
    "|:--   |:--      |:--    |:--      |\n",
    "| batch_norm_gradient_check.py | 배치 정규화를 구현한 신경망의 오차역전파법 방식의 기울기 계산이 정확한지 확인합니다(기울기 확인). |  |  |\n",
    "| batch_norm_test.py | MNIST 데이터셋 학습에 배치 정규화를 적용해봅니다. | 6.3.2 배치 정규화의 효과 | 212 |\n",
    "| hyperparameter_optimization.py | 무작위로 추출한 값부터 시작하여 두 하이퍼파라미터(가중치 감소 계수, 학습률)를 최적화해봅니다. | 6.5.3 하이퍼파라미터 최적화 구현하기 | 224 |\n",
    "| optimizer_compare_mnist.py | SGD, 모멘텀, AdaGrad, Adam의 학습 속도를 비교합니다. | 6.1.8 MNIST 데이터셋으로 본 갱신 방법 비교 | 201 |\n",
    "| optimizer_compare_naive.py | SGD, 모멘텀, AdaGrad, Adam의 학습 패턴을 비교합니다. | 6.1.7 어느 갱신 방법을 이용할 것인가? | 200 |\n",
    "| overfit_dropout.py | 일부러 오버피팅을 일으킨 후 드롭아웃(dropout)의 효과를 관찰합니다. | 6.4.3 드롭아웃 | 219 |\n",
    "| overfit_weight_decay.py | 일부러 오버피팅을 일으킨 후 가중치 감소(weight_decay)의 효과를 관찰합니다. | 6.4.1 오버피팅 | 215 |\n",
    "| weight_init_activation_histogram.py | 활성화 함수로 시그모이드 함수를 사용하는 5층 신경망에 무작위로 생성한 입력 데이터를 흘리며 각 층의 활성화값 분포를 히스토그램으로 그려봅니다. | 6.2.2 은닉층의 활성화값 분포 | 203 |\n",
    "| weight_init_compare.py | 가중치 초깃값(std=0.01, He, Xavier)에 따른 학습 속도를 비교합니다. | 6.2.4 MNIST 데이터셋으로 본 가중치 초깃값 비교 | 209 |\n",
    "-->\n",
    "\n",
    "\n",
    "---\n",
    "# 목차\n",
    "> 1. Holdout Validation\n",
    "    1. Basic holdout split\n",
    "    2. Per-class holdout split\n",
    "    3. Holdout split using random sampling\n",
    "    4. Holdout split using reproducible random sampling\n",
    "    5. Holdout split using stratified random sampling\n",
    "2. Cross Validation\n",
    "    1. k-fold Cross validation\n",
    "    2. Leave-one-out validation\n",
    "    3. Repeated random subsampling\n",
    "3. Three-Way Splitting\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "> ### Required Packages\n",
    "python 3.7 or higher, sys, os, numpy, sklearn, \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Holdout Validation\n",
    "    1. Basic holdout split\n",
    "    2. Per-class holdout split\n",
    "    3. Holdout split using random sampling\n",
    "    4. Holdout split using reproducible random sampling\n",
    "    5. Holdout split using stratified random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout: Overfitting\n",
    "\n",
    "- One may be tempted to use the entire training data to select the ``optimal'' classifier, then estimate the error rate\n",
    "- This naive approach has two fundamental problems\n",
    "    - The final model will normally __overfit__ to the training data;\n",
    "\t  it often occurs 100\\% correct classification on training data\n",
    "    - Then the model will not be able to generalize to new data\n",
    "\t- The problem of overfitting is more often with models\n",
    "\t  that have a large number of parameters\n",
    "\t- The error rate estimate is overly optimistic\n",
    "\t  (lower than the true error rate)\n",
    "- The techniques presented in this lecture will allow you to make the best use of your (limited) data for\n",
    "    1. Training\n",
    "    2. Model selection and\n",
    "\t3. Performance estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Holdout Method\n",
    "\n",
    "- Split dataset into two groups\n",
    "    - __Training set:__ used to train the classifier\n",
    "    - __Test set:__ used to estimate the error rate of the trained classifier\n",
    "   <img src=\"./images/fig_ho_01_train_test.png\" width=\"75%\" height=\"75%\"/>\n",
    "   > (a) Test set first\n",
    "   \n",
    "   <img src=\"./images/fig_ho_01_test_train.png\" width=\"75%\" height=\"75%\"/>\n",
    "   > (b) Train set first\n",
    "   \n",
    "- The holdout method has two basic drawbacks\n",
    "    1. In problems where we have a scarce dataset we may not be able to afford to set aside a portion of the dataset for testing\n",
    "    2. Since it is a single train-and-test experiment, the holdout estimate of error rate will be misleading for an __unfortunate__ split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HO1: Incorrect Holdout Split\n",
    "\n",
    "<img src=\"./images/fig_ho1_iris_incorrect.png\" width=\"75%\" height=\"75%\"/>\n",
    "\n",
    ">   Incorrect holdout split example on Iris dataset. \n",
    "    The data are split by test:train = 2:3.\n",
    "    There is no Setosa examples in the training set, \n",
    "    so the trained model cannot classify Setosa examples. \n",
    "    On the contracy, most examples of the test set belong to Setosa class, \n",
    "    so the test result will be biased too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_layer_net.py \n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# modified from train_neuralnet.py\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "def train_neuralnet_iris(x_train, t_train, x_test, t_test, \n",
    "                         input_size=4, hidden_size=10, output_size=3, \n",
    "                         iters_num = 1000, batch_size = 10, learning_rate = 0.1,\n",
    "                         verbose=True):\n",
    "    \n",
    "    network = TwoLayerNet(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Train Parameters\n",
    "    train_size = x_train.shape[0]\n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "    train_loss_list, train_acc_list, test_acc_list = [], [], []\n",
    "\n",
    "    for step in range(1, iters_num+1):\n",
    "        # get mini-batch\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = t_train[batch_mask]\n",
    "\n",
    "        # 기울기 계산\n",
    "        #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "        grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(압도적으로 빠르다)\n",
    "\n",
    "        # Update\n",
    "        for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "            network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "        # loss\n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "\n",
    "        if verbose and step % iter_per_epoch == 0:\n",
    "            train_acc = network.accuracy(x_train, t_train)\n",
    "            test_acc = network.accuracy(x_test, t_test)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            print('Step: {:4d}\\tTrain acc: {:.5f}\\tTest acc: {:.5f}'.format(step, \n",
    "                                                                            train_acc,\n",
    "                                                                            test_acc))\n",
    "    tracc, teacc = network.accuracy(x_train, t_train), network.accuracy(x_test, t_test)\n",
    "    if verbose:\n",
    "        print('Optimization finished!')\n",
    "        print('Training accuracy: %.2f' % tracc)\n",
    "        print('Test accuracy: %.2f' % teacc)\n",
    "    return tracc, teacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris = datasets.load_iris(); nsamples = iris.data.shape[0]\n",
    "ntestsamples = nsamples * 4 // 10  # `//' is integer division\n",
    "ntrainsamples = nsamples - ntestsamples\t  # 4:6 test:train split\n",
    "testidx = range(0,ntestsamples); trainidx = range(ntestsamples,nsamples)\n",
    "\n",
    "train_neuralnet_iris(iris.data[trainidx,:], iris.target[trainidx],\n",
    "                     iris.data[testidx], iris.target[testidx],\n",
    "                     input_size=4, hidden_size=10, output_size=3, \n",
    "                     iters_num = 1000, batch_size = 10, learning_rate = 0.1)\n",
    "\n",
    "# Although only 1 Verisicolour example out of 10 is mislabeled.\n",
    "# All 50 Setosa examples are mislabeled because there is no Setosa class in the model\n",
    "####################################\n",
    "\n",
    "\n",
    "\"\"\"'\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(iris.data[trainidx,:], iris.target[trainidx])\n",
    "y_pred = gnb.predict(iris.data[testidx,:])\n",
    "nmisses = (iris.target[testidx] != y_pred).sum()\n",
    "print('Number of mislabeled out of a total %d samples : %d (%.2f%%)'\n",
    "        % (len(testidx), nmisses, float(nmisses)/len(testidx)*100.0))\n",
    "# [Execution result]\n",
    "# Number of mislabeled out of a total 60 samples : 51 (85.00%)\n",
    "''\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HO2: Per-class Holdout Split\n",
    "\n",
    "<img src=\"./images/fig_ho2_iris_perclass.png\"/>\n",
    "\n",
    ">   Per-class holdout split example on Iris dataset. \n",
    "    The data are split by test:train = 2:3, and applied to 50 samples per class.\n",
    "    Test set is the union of the test sets from individual classes,\n",
    "    and training set is the union of the 3 training sets.\n",
    "    Equal number of examples are chosen for each of the classes\n",
    "    so that the test result will not be biased.\n",
    "\n",
    "- The splitting should be carefully designed so that at least 1 example for each of training and test set\n",
    "- As an extreme case, when there are only 2 examples, it is reasonable to assign one for each of train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "# 4:6 test:train split\n",
    "ntestsamples = len(iris.target) * 4 // 10  # '//' integer division\n",
    "ntestperclass = ntestsamples // 3\n",
    "\n",
    "# allocate indices for test and training data\n",
    "# Bte: boolean index for test data;  ~Bte: logical not, for training data\n",
    "Bte = np.zeros(len(iris.target),dtype=bool)   # initially, False index\n",
    "for c in range(0,3): Bte[range(c*50,c*50+ntestperclass)] = True\n",
    "\n",
    "train_neuralnet_iris(iris.data[~Bte,:], iris.target[~Bte],\n",
    "                     iris.data[Bte,:], iris.target[Bte],\n",
    "                     input_size=4, hidden_size=10, output_size=3, \n",
    "                     iters_num = 1000, batch_size = 10, learning_rate = 0.1)\n",
    "\n",
    "\n",
    "'''\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(iris.data[~Bte,:], iris.target[~Bte])\n",
    "y_pred = gnb.predict(iris.data[Bte,:])\n",
    "nmisses = (iris.target[Bte] != y_pred).sum()\n",
    "print('Number of mislabeled out of a total %d samples : %d (%.2f%%)'\n",
    "        % (sum(Bte), nmisses, float(nmisses)/sum(Bte)*100.0))\n",
    "\n",
    "#[Execution]\n",
    "#Number of mislabeled out of a total 60 samples : 3 (5.00%)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HO3: Holdout Split by Random Sampling\n",
    "\n",
    "<img src=\"./images/fig_ho3_iris_random.png\" width=\"100%\" height=\"100%\"/>\n",
    "\n",
    ">   Holdout split using random sampling Iris dataset. \n",
    "    The data are split by test:train = 2:3.\n",
    "    60 examples are selected randomly for the test set, \n",
    "    and 90 for the training set.\n",
    "- Each data split randomly selects a (fixed) number of examples\n",
    "  __without replacement__\n",
    "  --- _each sample has only one chance to be selected_\n",
    "- Drawbacks:\n",
    "    1. due to random selection, the performance may vary for different executions\n",
    "\t2. the number of examples per class may not be balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "nsamples = iris.data.shape[0]\n",
    "ntestsamples = nsamples * 4 // 10  # 4:6 test:train split\n",
    "# random permutation (shuffling)\n",
    "Irand = np.random.permutation(nsamples)\n",
    "Ite = Irand[range(0,ntestsamples)]\n",
    "Itr = Irand[range(ntestsamples,nsamples)]\n",
    "\n",
    "train_neuralnet_iris(iris.data[Itr,:], iris.target[Itr],\n",
    "                     iris.data[Ite,:], iris.target[Ite],\n",
    "                     input_size=4, hidden_size=10, output_size=3, \n",
    "                     iters_num = 1000, batch_size = 10, learning_rate = 0.1)\n",
    "\n",
    "'''\n",
    "# training and testing\n",
    "gnb = GaussianNB().fit(iris.data[Itr,:], iris.target[Itr])\n",
    "y_pred = gnb.predict(iris.data[Ite,:])\n",
    "nmisses = (iris.target[Ite] != y_pred).sum()\n",
    "print('Number of mislabeled out of a total %d samples : %d (%.2f%%)'\n",
    "        % (sum(Ite), nmisses, float(nmisses)/sum(Ite)*100.0))\n",
    "\n",
    "# [Multiple Executions]\n",
    "# Number of mislabeled out of a total 60 samples : 3 (5.00%)\n",
    "# Number of mislabeled out of a total 60 samples : 4 (6.67%)\n",
    "# Number of mislabeled out of a total 60 samples : 0 (0.00%)\n",
    "# Number of mislabeled out of a total 60 samples : 5 (8.33%)\n",
    "# Number of mislabeled out of a total 60 samples : 1 (1.67%)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.model\\_selection.train\\_test\\_split\n",
    "\n",
    "<img src=\"./images/train_test_split.png\" width=\"100%\" height=\"100%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling by sklearn.model_selection.train_test_split\n",
    "# source: https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We can now quickly sample a training set while holding out \n",
    "# 40% of the data for testing (evaluating) our classifier:\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "Xtr,Xte,ytr,yte = train_test_split(X, y, test_size=0.4, shuffle=True)\n",
    "\n",
    "train_neuralnet_iris(Xtr,ytr,Xte,yte,\n",
    "                     input_size=4, hidden_size=10, output_size=3, \n",
    "                     iters_num = 1000, batch_size = 10, learning_rate = 0.1)\n",
    "\n",
    "\n",
    "'''\n",
    "# training and testing \n",
    "y_pred = GaussianNB().fit(Xtr, ytr).predict(Xte)\n",
    "nmisses = (yte != y_pred).sum()\n",
    "print('Number of mislabeled out of a total %d samples : %d (%.2f%%)'\n",
    "        % (len(yte), nmisses, float(nmisses)/len(yte)*100.0))\n",
    "\n",
    "# [Multiple Executions]\n",
    "# Number of mislabeled out of a total 60 samples : 5 (8.33%)\n",
    "# Number of mislabeled out of a total 60 samples : 4 (6.67%)\n",
    "# Number of mislabeled out of a total 60 samples : 2 (3.33%)\n",
    "# Number of mislabeled out of a total 60 samples : 6 (10.00%)\n",
    "# Number of mislabeled out of a total 60 samples : 1 (1.67%)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Random Sampling Holdout\n",
    "\n",
    "<img src=\"./images/train_test_split_random_state.png\" width=\"100%\" height=\"100%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HO4: Reproducible Random Sampling\n",
    "# Random sampling by sklearn.model_selection.train_test_split\n",
    "# source: https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "Xtr,Xte,ytr,yte = train_test_split(X, y, test_size=0.4, shuffle=True, random_state=len(y))\n",
    "\n",
    "# fix the SEED of random permutation to be the number of samples, \n",
    "# to reproduce the same random sequence at every execution\n",
    "np.random.seed(len(y))\n",
    "\n",
    "train_neuralnet_iris(Xtr,ytr,Xte,yte,\n",
    "                     input_size=4, hidden_size=10, output_size=3, \n",
    "                     iters_num = 1000, batch_size = 10, learning_rate = 0.1)\n",
    "\n",
    "\"\"\" # training and testing \n",
    "y_pred = GaussianNB().fit(Xtr, ytr).predict(Xte)\n",
    "nmisses = (yte != y_pred).sum()\n",
    "print('Number of mislabeled out of a total %d samples : %d (%.2f%%)'\n",
    "        % (len(yte), nmisses, float(nmisses)/len(yte)*100.0))\n",
    "\n",
    "# [Multiple Executions]\n",
    "# Number of mislabeled out of a total 60 samples : 3 (5.00%)\n",
    "# Number of mislabeled out of a total 60 samples : 3 (5.00%)\n",
    "# Number of mislabeled out of a total 60 samples : 3 (5.00%)\n",
    "# Number of mislabeled out of a total 60 samples : 3 (5.00%)\n",
    "# Number of mislabeled out of a total 60 samples : 3 (5.00%)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Random Sampling Holdout (2)\n",
    "\n",
    "<img src=\"./images/train_test_split_stratified.png\" width=\"100%\" height=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Random Sampling\n",
    "From (Wikipedia)[https://en.wikipedia.org/wiki/Stratified_sampling]\n",
    "- A method of sampling from a population which can be partitioned into subpopulations.\n",
    "- When subpopulations within an overall population vary, it could be advantageous to sample each subpopulation (stratum) independently. \n",
    "- Stratification is the process of dividing members of the population into homogeneous subgroups before sampling. \n",
    "- The ratio of each category in the sample space is retained in the sub-sampled space.\n",
    "<img src=\"./images/fig_stratified_sampling.png\" width=\"100%\" height=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HO5: Stratified Random Sampling\n",
    "\n",
    "<img src=\"./images/fig_ho5_iris_stratified_random.png\" width=\"100%\" height=\"100%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HO5: Stratified Random Sampling}\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# per-class random sampling by passing y to variable stratify, \n",
    "Xtr,Xte,ytr,yte = train_test_split(X, y, test_size=0.4, shuffle=True, stratify=y)\n",
    "\n",
    "# check number of samples of the individual classes\n",
    "print('test: %d %d %d,  '%(sum(yte==0),sum(yte==1),sum(yte==2)),end='')\n",
    "print('training: %d %d %d'%(sum(ytr==0),sum(ytr==1),sum(ytr==2)))\n",
    "\n",
    "# due to the random initialization of the weights, the performance varies\n",
    "# so we have to set the random seed for TwoLayerNet's initialization values\n",
    "np.random.seed(len(y))\n",
    "\n",
    "train_neuralnet_iris(Xtr,ytr,Xte,yte,\n",
    "                     input_size=4, hidden_size=10, output_size=3, \n",
    "                     iters_num = 1000, batch_size = 10, learning_rate = 0.1)\n",
    "\n",
    "\"\"\"# training and testing \n",
    "y_pred = GaussianNB().fit(Xtr, ytr).predict(Xte)\n",
    "nmisses = (yte != y_pred).sum()\n",
    "print('Number of mislabeled out of a total %d samples : %d (%.2f%%)'\n",
    "        % (len(yte), nmisses, float(nmisses)/len(yte)*100.0))\n",
    "\n",
    "# [Multiple Executions]\n",
    "# test: 20 20 20,  training: 30 30 30\n",
    "# Number of mislabeled out of a total 60 samples : 6 (10.00%)\n",
    "# test: 20 20 20,  training: 30 30 30\n",
    "# Number of mislabeled out of a total 60 samples : 3 (5.00%)\n",
    "# test: 20 20 20,  training: 30 30 30\n",
    "# Number of mislabeled out of a total 60 samples : 4 (6.67%)\n",
    "# test: 20 20 20,  training: 30 30 30\n",
    "# Number of mislabeled out of a total 60 samples : 3 (5.00%)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Cross Validation\n",
    "    1. k-fold Cross validation\n",
    "    2. Leave-one-out validation\n",
    "    3. Repeated random subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Limitation of Holdout Method\n",
    "- The training and test data are fixed in the holdout, so the measured performance is highly dependent on the choice of the test set\n",
    "- The limitations of the holdout, especially with the lack of training data, can be overcome with a family of resampling methods at the expense of higher computational cost\n",
    "- Types of validation methods\n",
    "    1. $K$-fold crossvalidation (KFCV)\n",
    "\t2. Leave-one-out cross-validation (LOOCV)\n",
    "\t3. Repeated random subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Subsampling: $K$ Data Splits\n",
    "\n",
    "### Performing $K$ data splits of the entire dataset\n",
    "\n",
    "- Each data split randomly selects a (fixed) number of examples _without replacement_ --- _each sample has only one chance to be selected_\n",
    "- For each data split we retrain the classifier from scratch with the (unselected) training examples and then estimate the error, $E_{i}$, on the (selected) test examples\n",
    "- The true error estimate is obtained as the average of the separate estimates\n",
    "$$E_{i}$: $E = \\frac{1}{K} \\sum_{i=1}^{K} E_{i}$$\n",
    "- This estimate is significantly better than the holdout estimate\n",
    "\n",
    "<img src=\"images/fig_cv_02_randomsubsampling.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated Random Subsampling\n",
    "# Repeating stratified random sampling K times\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# due to the random initialization of the weights, the performance varies\n",
    "# so we have to set the random seed for TwoLayerNet's initialization values\n",
    "np.random.seed(len(y))\n",
    "\n",
    "K = 20\n",
    "Acc = np.zeros([K,2], dtype=float)\n",
    "for k in range(K):\n",
    "    # stratified random sampling\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.4, shuffle=True, random_state=None, stratify=y)\n",
    "    Acc[k,0], Acc[k,1] = train_neuralnet_iris(Xtr,ytr,Xte,yte,\n",
    "                                  input_size=4, hidden_size=10, output_size=3, \n",
    "                                  iters_num = 1000, batch_size = 10, learning_rate = 0.1, \n",
    "                                  verbose = False)\n",
    "    print('Trial %d: accuracy %.3f %.3f' % (k, Acc[k,0], Acc[k,1]))\n",
    "\n",
    "# 20 trials, average mislabeled out of a total 60 samples : 2.6 (4.42%)\n",
    "# 20 trials, average mislabeled out of a total 60 samples : 2.8 (4.67%)\n",
    "# 20 trials, average mislabeled out of a total 60 samples : 2.6 (4.33%)\n",
    "# 20 trials, average mislabeled out of a total 60 samples : 2.8 (4.58%)\n",
    "# 20 trials, average mislabeled out of a total 60 samples : 3.0 (5.08%)\n",
    "# 20 trials, average mislabeled out of a total 60 samples : 2.7 (4.50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $K$-fold Cross Validation (KFCV)\n",
    "\n",
    "- Create a $K$-fold partition of the dataset\n",
    "    - For each of $K$ experiments, use $K-1$ folds for training and the remaining fold for testing\n",
    "- $K$-Fold cross validation is similar to random subsampling\n",
    "    - The advantage of KFCV is that all the examples in the dataset are eventually used for both training and testing\n",
    "    - The true error is estimated as the average error rate\n",
    "\t  on test examples: $E = \\frac{1}{K} \\sum_{i=1}^{K} E_{i}$\n",
    "<img src=\"./images/fig_cv_03_4FCV.png\" width=\"100%\" height=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many folds are needed?\n",
    "- With a large number of folds\n",
    "    - $(+)$ The bias of the true error rate estimator is small (accurate estimator)\n",
    "\t- $(-)$ The variance of the true error rate estimator is large\n",
    "\t- $(-)$ The computational time is very large as well (many experiments)\n",
    "- With a small number of folds\n",
    "    - $(+)$ The number of experiments and, therefore, computation time are reduced\n",
    "\t- $(+)$ The variance of the estimator is small\n",
    "\t- $(-)$ The bias of the estimator is large (larger than the true error rate)\n",
    "      \\end{itemize}\n",
    "- In practice, the choice for $K$ depends on the size of the dataset\n",
    "    1. For large datasets, even 3-fold cross validation will be quite accurate\n",
    "\t2. For very sparse datasets, we may have to use leave-one-out in order to train on as many examples as possible\n",
    "- A common choise is $K=10$ for moderately sized dataset, and $K=5$ for small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out Cross Validation\n",
    "__LOO__ is the degenerate case of KFCV, where $K$ is chosen as the total number of examples\n",
    "- For a dataset with $N$ examples, perform $N$ experiments\n",
    "- For each experiment use $N-1$ examples for training and the remaining single example for testing\n",
    "- The true error is estimated as the average error rate on test examples: \n",
    "$$E = \\frac{1}{\\underline{N}} \\sum_{i=1}^{\\underline{N}} E_{i}$$\n",
    "<img src=\"./images/fig_cv_04_LOO.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOO is useful for $k$-NN, because no model training is required.\n",
    "# File name: knn_iris_loo_skl.py\n",
    "# Required Package(s): sklearn numpy\n",
    "# Description: LOO (leave-one-out) cross validation, k nearest neighbors on IRIS\n",
    "\n",
    "# modified from https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "for k in [1,3,5,7,9]:\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    I = np.ones(iris.target.shape,dtype=bool)   # True index array\n",
    "    y_pred = -np.ones(iris.target.shape,dtype=int)    # prediction, assigned -1 for initial values\n",
    "    for n in range(len(iris.target)):\n",
    "        I[n] = False    # unselect, leave one\n",
    "        y_pred[n] = neigh.fit(iris.data[I,:], iris.target[I]).predict(iris.data[n,:].reshape(1,-1))\n",
    "        I[n] = True     # select, for the next step\n",
    "\n",
    "    nsamples = iris.data.shape[0]\n",
    "    nmisses = (iris.target != y_pred).sum()\n",
    "    print('kNN with k=%d' % k)\n",
    "    print('Number of mislabeled out of a total %d samples : %d (%.2f%%)'\n",
    "            % (nsamples, nmisses, float(nmisses)/float(nsamples)*100.0))\n",
    "\n",
    "# 1-NN, mislabeled out of a 150 samples : 6 (4.00%)\n",
    "# 3-NN, mislabeled out of a 150 samples : 6 (4.00%)\n",
    "# 5-NN, mislabeled out of a 150 samples : 5 (3.33%)\n",
    "# 7-NN, mislabeled out of a 150 samples : 5 (3.33%)\n",
    "# 9-NN, mislabeled out of a 150 samples : 5 (3.33%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "- The Bootstrap: resampling _with replacement_\n",
    "- From a dataset with $N$ examples\n",
    "    1. Randomly select (_with replacement_) $N$ examples and use this set for training\n",
    "    2. The remaining examples not selected for training are used for testing\n",
    "\t3. This value is likely to change from fold to fold \n",
    "- Repeat this process for a specified number of folds ($K$)\n",
    "- The true error is estimated as the average error rate on test data\n",
    "\n",
    "<img src=\"./images/fig_cv_05_bootstrap.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of CV and Boostrapping\n",
    "- Compared to basic CV, the bootstrap increases the variance that can occur in each fold [Efron and Tibshirani, 1994]\n",
    "    - This is a desirable property since it is a more realistic simulation of the real-life experiment from which our dataset was obtained\n",
    "- Consider a classification problem with $C$ classes, a total of $N$ examples and $N_{c}$ examples for each class $c$ \n",
    "    1. The _a priori_ probability of choosing an example from class $c$ is $N_{c}/N$\n",
    "        - Once we choose an example from class $c$, \n",
    "\t      if we do not replace it for the next selection,\n",
    "\t      then the \\emph{a priori} probabilities will have changed\n",
    "\t      since the probability of choosing an example from\n",
    "\t      class $c$ will now be $(N_{c}-1)/N$\n",
    "    2. Thus, sampling with replacement preserves\n",
    "\t  the _a priori_ probabilities of the classes\n",
    "\t  throughout the random selection process\n",
    "\t3. An additional benefit is that the bootstrap can provide\n",
    "\t  accurate measures of BOTH the bias and variance of\n",
    "\t  the true error estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Part 3 Three-way Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two issues arise for machine learning system design\n",
    "- __Selection:__ How do we select the _optimal_ parameter(s) for a given problem?\n",
    "- __Validation:__ Once we have chosen a model, how to estimate its _TRUE_ error rate?\n",
    "    - The true error rate is the error rate of the classifier when tested on the _ENTIRE POPULATION_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Limited Number of Examples\n",
    "- If we had access to an unlimited number of examples, these questions would have a straightforward answer\n",
    "    1. Choose the model that provides the lowest error rate on the entire population\n",
    "\t2. And, of course, that error rate is the true error rate\n",
    "- However, in real applications only a finite set of examples is available\n",
    "    1. This number is usually smaller than that we would hope for\n",
    "\t2. Data collection is a very expensive process\n",
    "- The model from the real data is an estimate of the true one\n",
    "    - As a compromise, find a model as close as possible given the limited amount of data\n",
    "\n",
    "\n",
    "## Three-way data splits\n",
    "- If model selection __and__ true error estimates are to be computed simultaneously, the data should be divided into three disjoint sets [Ripley, 1996]\n",
    "    - __Training set__ used for learning, e.g., to fit the parameters of the classifier\n",
    "        - In an MLP, use the training set to find the __optimal__ weights with the back-propagation learning rule\n",
    "    - __Validation set__ used to select among several trained classifiers\n",
    "        - In an MLP, use the validation set to find the __optimal__ number of hidden units or determine a stopping point for the back-propagation algorithm\n",
    "    - __Test set__ used only to assess the performance of a fully-trained classifier\n",
    "        - In an MLP, use the test to estimate the error rate after we have chosen the final model (MLP size and actual weights)\n",
    "\n",
    "- Why separate test and validation sets?\n",
    "    - The error rate of the final model on validation data will be biased (smaller than the true error rate) since the validation set is used to select the final model\n",
    "    - After assessing the final model on the test set, YOU MUST NOT tune the model any further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three-way data splitting procedure\n",
    "1. Divide the available data into training, validation and test set\n",
    "1. Select architecture and training parameters\n",
    "1. Train the model using the training set\n",
    "1. Evaluate the model using the validation set\n",
    "1. Repeat steps 2 through 4 using different architectures and training parameters\n",
    "1. Select the best model and train it using data\n",
    "1. the training and validation sets\n",
    "1. Assess this final model using the test set\n",
    "\n",
    "- This outline assumes a holdout method\n",
    "- If CV or bootstrap are used, steps 3 and 4 have to be repeated for each of the $K$ folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameter Estimation Workflow\n",
    "<img src=\"./images/grid_search_workflow.png\"/>\n",
    "\n",
    "## Dataflow in Three-Way Splits\n",
    "<img src=\"./images/fig_cv_06_3waysplit.png\"/>\n",
    "\n",
    "## Three-Way Split with Cross Validation\n",
    "<img src=\"./images/grid_search_cross_validation.png\"/>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
